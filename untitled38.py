# -*- coding: utf-8 -*-
"""Untitled38.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U2YUtVMfnPYpMT_r_zGoOeNXYzn7SJmc
"""

# Full Attention-LSTM Project Code
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Layer, Multiply, Softmax
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings("ignore")

# -------------------------------
# 1. Generate Synthetic Multivariate Time Series
# -------------------------------
np.random.seed(42)
n_obs = 500
t = np.arange(n_obs)

# Three variables with trend + seasonality + noise
Var1 = 0.05*t + 2*np.sin(0.1*t) + 0.5*np.random.randn(n_obs)
Var2 = 0.03*t + 1.5*np.cos(0.08*t) + 0.3*np.random.randn(n_obs)
Var3 = -0.02*t + np.sin(0.12*t) + 0.4*np.random.randn(n_obs)

df = pd.DataFrame({"Var1": Var1, "Var2": Var2, "Var3": Var3})

# Scale data
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df)

# -------------------------------
# 2. Create Sequences for LSTM
# -------------------------------
def create_sequences(data, seq_len=10):
    X, y = [], []
    for i in range(len(data)-seq_len):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len])
    return np.array(X), np.array(y)

seq_len = 20
X, y = create_sequences(scaled_data, seq_len)
split = int(0.8*len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# -------------------------------
# 3. Define Custom Attention Layer
# -------------------------------
class Attention(Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1]),
                                 initializer="glorot_uniform",
                                 trainable=True)
        self.b = self.add_weight(shape=(input_shape[-1],),
                                 initializer="zeros",
                                 trainable=True)
        self.u = self.add_weight(shape=(input_shape[-1], 1),
                                 initializer="glorot_uniform",
                                 trainable=True)
        super(Attention, self).build(input_shape)

    def call(self, inputs):
        # Score computation
        u_it = tf.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)
        score = tf.tensordot(u_it, self.u, axes=1)
        attention_weights = tf.nn.softmax(score, axis=1)
        context_vector = tf.reduce_sum(inputs * attention_weights, axis=1)
        return context_vector

# -------------------------------
# 4. Build Attention-LSTM Model
# -------------------------------
inputs = Input(shape=(seq_len, X_train.shape[2]))
lstm_out = LSTM(64, return_sequences=True)(inputs)
attention_out = Attention()(lstm_out)
outputs = Dense(X_train.shape[2])(attention_out)
model = Model(inputs=inputs, outputs=outputs)
model.compile(optimizer='adam', loss='mse')
model.summary()

# -------------------------------
# 5. Train Model
# -------------------------------
history = model.fit(X_train, y_train, epochs=50, batch_size=16,
                    validation_split=0.1, verbose=0)

# -------------------------------
# 6. Baseline LSTM Model
# -------------------------------
baseline_input = Input(shape=(seq_len, X_train.shape[2]))
baseline_lstm = LSTM(64)(baseline_input)
baseline_output = Dense(X_train.shape[2])(baseline_lstm)
baseline_model = Model(inputs=baseline_input, outputs=baseline_output)
baseline_model.compile(optimizer='adam', loss='mse')
baseline_model.fit(X_train, y_train, epochs=50, batch_size=16,
                   validation_split=0.1, verbose=0)

# -------------------------------
# 7. Predictions & Evaluation
# -------------------------------
y_pred_attention = model.predict(X_test)
y_pred_baseline = baseline_model.predict(X_test)

# Rescale back
y_test_rescaled = scaler.inverse_transform(y_test)
y_attention_rescaled = scaler.inverse_transform(y_pred_attention)
y_baseline_rescaled = scaler.inverse_transform(y_pred_baseline)

def evaluate(true, pred):
    mse = mean_squared_error(true, pred)
    mae = mean_absolute_error(true, pred)
    return mse, mae

mse_att, mae_att = evaluate(y_test_rescaled, y_attention_rescaled)
mse_base, mae_base = evaluate(y_test_rescaled, y_baseline_rescaled)

print("\nForecasting Performance:")
print(f"Attention-LSTM: MSE={mse_att:.4f}, MAE={mae_att:.4f}")
print(f"Baseline LSTM:  MSE={mse_base:.4f}, MAE={mae_base:.4f}")

# -------------------------------
# 8. Plot Forecasts for Var1
# -------------------------------
plt.figure(figsize=(10,6))
plt.plot(y_test_rescaled[:,0], label="True Var1", marker='o')
plt.plot(y_attention_rescaled[:,0], label="Attention-LSTM Var1", marker='x')
plt.plot(y_baseline_rescaled[:,0], label="Baseline LSTM Var1", marker='s')
plt.title("Forecast Comparison for Var1")
plt.xlabel("Time Step")
plt.ylabel("Value")
plt.legend()
plt.show()